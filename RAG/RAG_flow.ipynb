{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b483017",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "### This RAG flow consists of the following steps:\n",
    "\n",
    "1. **Text Chunking**: The text is split into smaller chunks for better processing.\n",
    "2. **Text Embedding**: Each chunk is converted into a vector representation.\n",
    "3. **Vector Database**: The vectors are stored in a database for efficient retrieval.\n",
    "4. **Best Match 25**: When a query is made, the system retrieves the 25 most relevant chunks based on the vector similarity.\n",
    "5. **Hybrid Search**: The system performs a hybrid search combining vector similarity and keyword matching to find the most relevant chunks.\n",
    "6. **Re-ranking**: The retrieved chunks are re-ranked based on their relevance to the query.\n",
    "7. **Contextual Answer Generation**: The final answer is generated using the most relevant chunks, providing a contextually rich response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6013616",
   "metadata": {},
   "source": [
    "# dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031b5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 20:07:06 - __main__ - INFO - Gemini client created successfully.\n",
      "2025-08-08 20:07:06 - __main__ - INFO - Anthropic client created successfully.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from anthropic import Anthropic\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Callable, Any, List, Dict, Tuple, Optional, Protocol\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    gemini_client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    logger.info(\"Gemini client created successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to create Gemini client: %s\", e)\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    anthropic_client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    logger.info(\"Anthropic client created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Anthropic client: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a43a0",
   "metadata": {},
   "source": [
    "# global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2a8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"gemini-embedding-001\"\n",
    "ANTHROPIC_MODEL=\"claude-3-haiku-20240307\"\n",
    "TEMPERATURE=0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc4f38",
   "metadata": {},
   "source": [
    "# prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d645404",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_PROMT = \"\"\"\n",
    "Based on the following context, please answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based on the context provided.\n",
    "If the context doesn't contain enough information to answer the question,\n",
    "please say so clearly.\n",
    "\"\"\"\n",
    "\n",
    "RERANK_PROMPT = \"\"\"\n",
    "You are about to be given a set of documents, along with an id of each.\n",
    "Your task is to select the {k} most relevant documents to answer the user's question.\n",
    "\n",
    "Here is the user's question:\n",
    "<question>\n",
    "{query_text}\n",
    "</question>\n",
    "\n",
    "Here are the documents to select from:\n",
    "<documents>\n",
    "{joined_docs}\n",
    "</documents>\n",
    "\n",
    "Respond in the following format:\n",
    "```json\n",
    "{{\n",
    "\"document_ids\": [] \n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "CONTEXT_PROMT = \"\"\"\n",
    "Write a short and succinct snippet of text to situate this chunk within the \n",
    "overall source document for the purposes of improving search retrieval of the chunk. \n",
    "\n",
    "Here is the original source document:\n",
    "<document> \n",
    "{source_text}\n",
    "</document> \n",
    "\n",
    "Here is the chunk we want to situate within the whole document:\n",
    "<chunk> \n",
    "{text_chunk}\n",
    "</chunk>\n",
    "\n",
    "Answer only with the succinct context and nothing else. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fdaf4",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c6d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_message(messages, message_content):\n",
    "    if isinstance(message_content, list):\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message_content\n",
    "        }\n",
    "    else:\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": str(message_content)}]}\n",
    "\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, message_content):\n",
    "    if isinstance(message_content, list):\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": message_content\n",
    "        }\n",
    "    elif hasattr(message_content, \"content\"):\n",
    "        content_list = []\n",
    "        for block in message_content.content:\n",
    "            if block.type == \"text\":\n",
    "                content_list.append({\"type\": \"text\", \"text\": block.text})\n",
    "            elif block.type == \"tool_use\":\n",
    "                content_list.append({\n",
    "                    \"type\": \"tool_use\",\n",
    "                    \"id\": block.id,\n",
    "                    \"name\": block.name,\n",
    "                    \"input\": block.input\n",
    "                })\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": content_list\n",
    "        }\n",
    "    else:\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": message_content}]\n",
    "        }\n",
    "\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "def chat(messages, model=ANTHROPIC_MODEL, temperature=TEMPERATURE, system=None, stop_sequences=None, tools=None, tool_choice=None, betas=[]):\n",
    "    try:\n",
    "        params = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": 1000,\n",
    "        }\n",
    "        if system:\n",
    "            params[\"system\"] = system\n",
    "\n",
    "        if tools:\n",
    "            params[\"tools\"] = tools\n",
    "\n",
    "        if tool_choice:\n",
    "            params[\"tool_choice\"] = tool_choice\n",
    "\n",
    "        if betas:\n",
    "            params[\"betas\"] = betas\n",
    "\n",
    "        if stop_sequences: \n",
    "            params[\"stop_sequences\"] = stop_sequences\n",
    "\n",
    "        return anthropic_client.messages.create(**params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat streaming failed: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def text_from_message(message):\n",
    "    return \"\\n\".join(\n",
    "        [block.text for block in message.content if block.type == \"text\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9e7e2",
   "metadata": {},
   "source": [
    "# 1. text chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87f4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_structure(text):\n",
    "    pattern = r\"\\n## \"\n",
    "\n",
    "    return re.split(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dbc2e9",
   "metadata": {},
   "source": [
    "# 2. text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f1f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model=EMBEDDING_MODEL):\n",
    "    try:\n",
    "        response = gemini_client.models.embed_content(\n",
    "            model=model,\n",
    "            contents=text\n",
    "        )\n",
    "        embeddings = response.embeddings\n",
    "        logger.info(\"Embeddings generated successfully.\")\n",
    "        \n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to generate embeddings: %s\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823daf7",
   "metadata": {},
   "source": [
    "# 3. vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61236093",
   "metadata": {},
   "source": [
    "## VectorIndex implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c0cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorIndex:\n",
    "    def __init__(\n",
    "        self,\n",
    "        distance_metric: str = \"cosine\",\n",
    "        embedding_fn=None,\n",
    "    ):\n",
    "        self.vectors: List[List[float]] = []\n",
    "        self.documents: List[Dict[str, Any]] = []\n",
    "        self._vector_dim: Optional[int] = None\n",
    "        if distance_metric not in [\"cosine\", \"euclidean\"]:\n",
    "            raise ValueError(\"distance_metric must be 'cosine' or 'euclidean'\")\n",
    "        self._distance_metric = distance_metric\n",
    "        self._embedding_fn = embedding_fn\n",
    "\n",
    "    def add_document(self, document: Dict[str, Any]):\n",
    "        if not self._embedding_fn:\n",
    "            raise ValueError(\n",
    "                \"Embedding function not provided during initialization.\"\n",
    "            )\n",
    "        if not isinstance(document, dict):\n",
    "            raise TypeError(\"Document must be a dictionary.\")\n",
    "        if \"content\" not in document:\n",
    "            raise ValueError(\n",
    "                \"Document dictionary must contain a 'content' key.\"\n",
    "            )\n",
    "\n",
    "        content = document[\"content\"]\n",
    "        if not isinstance(content, str):\n",
    "            raise TypeError(\"Document 'content' must be a string.\")\n",
    "\n",
    "        vector = self._embedding_fn(content)\n",
    "        self.add_vector(vector=vector, document=document)\n",
    "\n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        if not self._embedding_fn:\n",
    "            raise ValueError(\n",
    "                \"Embedding function not provided during initialization.\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(documents, list):\n",
    "            raise TypeError(\"Documents must be a list of dictionaries.\")\n",
    "\n",
    "        if not documents:\n",
    "            return\n",
    "\n",
    "        contents = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            if not isinstance(doc, dict):\n",
    "                raise TypeError(f\"Document at index {i} must be a dictionary.\")\n",
    "            if \"content\" not in doc:\n",
    "                raise ValueError(\n",
    "                    f\"Document at index {i} must contain a 'content' key.\"\n",
    "                )\n",
    "            if not isinstance(doc[\"content\"], str):\n",
    "                raise TypeError(\n",
    "                    f\"Document 'content' at index {i} must be a string.\"\n",
    "                )\n",
    "            contents.append(doc[\"content\"])\n",
    "\n",
    "        vectors = self._embedding_fn(contents)\n",
    "\n",
    "        for vector, document in zip(vectors, documents):\n",
    "            self.add_vector(vector=vector, document=document)\n",
    "\n",
    "    def search(\n",
    "        self, query: Any, k: int = 1\n",
    "    ) -> List[Tuple[Dict[str, Any], float]]:\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "\n",
    "        if isinstance(query, str):\n",
    "            if not self._embedding_fn:\n",
    "                raise ValueError(\n",
    "                    \"Embedding function not provided for string query.\"\n",
    "                )\n",
    "            query_vector = self._embedding_fn(query)\n",
    "        elif isinstance(query, list) and all(\n",
    "            isinstance(x, (int, float)) for x in query\n",
    "        ):\n",
    "            query_vector = query\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Query must be either a string or a list of numbers.\"\n",
    "            )\n",
    "\n",
    "        if self._vector_dim is None:\n",
    "            return []\n",
    "\n",
    "        if len(query_vector) != self._vector_dim:\n",
    "            raise ValueError(\n",
    "                f\"Query vector dimension mismatch. Expected {self._vector_dim}, got {len(query_vector)}\"\n",
    "            )\n",
    "\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k must be a positive integer.\")\n",
    "\n",
    "        if self._distance_metric == \"cosine\":\n",
    "            dist_func = self._cosine_distance\n",
    "        else:\n",
    "            dist_func = self._euclidean_distance\n",
    "\n",
    "        distances = []\n",
    "        for i, stored_vector in enumerate(self.vectors):\n",
    "            distance = dist_func(query_vector, stored_vector)\n",
    "            distances.append((distance, self.documents[i]))\n",
    "\n",
    "        distances.sort(key=lambda item: item[0])\n",
    "\n",
    "        return [(doc, dist) for dist, doc in distances[:k]]\n",
    "\n",
    "    def add_vector(self, vector, document: Dict[str, Any]):\n",
    "        if not isinstance(vector, list) or not all(\n",
    "            isinstance(x, (int, float)) for x in vector\n",
    "        ):\n",
    "            raise TypeError(\"Vector must be a list of numbers.\")\n",
    "        if not isinstance(document, dict):\n",
    "            raise TypeError(\"Document must be a dictionary.\")\n",
    "        if \"content\" not in document:\n",
    "            raise ValueError(\n",
    "                \"Document dictionary must contain a 'content' key.\"\n",
    "            )\n",
    "\n",
    "        if not self.vectors:\n",
    "            self._vector_dim = len(vector)\n",
    "        elif len(vector) != self._vector_dim:\n",
    "            raise ValueError(\n",
    "                f\"Inconsistent vector dimension. Expected {self._vector_dim}, got {len(vector)}\"\n",
    "            )\n",
    "\n",
    "        self.vectors.append(list(vector))\n",
    "        self.documents.append(document)\n",
    "\n",
    "    def _euclidean_distance(\n",
    "        self, vec1: List[float], vec2: List[float]\n",
    "    ) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "        return math.sqrt(sum((p - q) ** 2 for p, q in zip(vec1, vec2)))\n",
    "\n",
    "    def _dot_product(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "        return sum(p * q for p, q in zip(vec1, vec2))\n",
    "\n",
    "    def _magnitude(self, vec: List[float]) -> float:\n",
    "        return math.sqrt(sum(x * x for x in vec))\n",
    "\n",
    "    def _cosine_distance(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        if len(vec1) != len(vec2):\n",
    "            raise ValueError(\"Vectors must have the same dimension\")\n",
    "\n",
    "        mag1 = self._magnitude(vec1)\n",
    "        mag2 = self._magnitude(vec2)\n",
    "\n",
    "        if mag1 == 0 and mag2 == 0:\n",
    "            return 0.0\n",
    "        elif mag1 == 0 or mag2 == 0:\n",
    "            return 1.0\n",
    "\n",
    "        dot_prod = self._dot_product(vec1, vec2)\n",
    "        cosine_similarity = dot_prod / (mag1 * mag2)\n",
    "        cosine_similarity = max(-1.0, min(1.0, cosine_similarity))\n",
    "\n",
    "        return 1.0 - cosine_similarity\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.vectors)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        has_embed_fn = \"Yes\" if self._embedding_fn else \"No\"\n",
    "        return f\"VectorIndex(count={len(self)}, dim={self._vector_dim}, metric='{self._distance_metric}', has_embedding_fn='{has_embed_fn}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f22da",
   "metadata": {},
   "source": [
    "# 4. best match 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451be40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Index:\n",
    "    def __init__(\n",
    "        self,\n",
    "        k1: float = 1.5,\n",
    "        b: float = 0.75,\n",
    "        tokenizer: Optional[Callable[[str], List[str]]] = None,\n",
    "    ):\n",
    "        self.documents: List[Dict[str, Any]] = []\n",
    "        self._corpus_tokens: List[List[str]] = []\n",
    "        self._doc_len: List[int] = []\n",
    "        self._doc_freqs: Dict[str, int] = {}\n",
    "        self._avg_doc_len: float = 0.0\n",
    "        self._idf: Dict[str, float] = {}\n",
    "        self._index_built: bool = False\n",
    "\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self._tokenizer = tokenizer if tokenizer else self._default_tokenizer\n",
    "\n",
    "    def _default_tokenizer(self, text: str) -> List[str]:\n",
    "        text = text.lower()\n",
    "        tokens = re.split(r\"\\W+\", text)\n",
    "        return [token for token in tokens if token]\n",
    "\n",
    "    def _update_stats_add(self, doc_tokens: List[str]):\n",
    "        self._doc_len.append(len(doc_tokens))\n",
    "\n",
    "        seen_in_doc = set()\n",
    "        for token in doc_tokens:\n",
    "            if token not in seen_in_doc:\n",
    "                self._doc_freqs[token] = self._doc_freqs.get(token, 0) + 1\n",
    "                seen_in_doc.add(token)\n",
    "\n",
    "        self._index_built = False\n",
    "\n",
    "    def _calculate_idf(self):\n",
    "        N = len(self.documents)\n",
    "        self._idf = {}\n",
    "        for term, freq in self._doc_freqs.items():\n",
    "            idf_score = math.log(((N - freq + 0.5) / (freq + 0.5)) + 1)\n",
    "            self._idf[term] = idf_score\n",
    "\n",
    "    def _build_index(self):\n",
    "        if not self.documents:\n",
    "            self._avg_doc_len = 0.0\n",
    "            self._idf = {}\n",
    "            self._index_built = True\n",
    "            return\n",
    "\n",
    "        self._avg_doc_len = sum(self._doc_len) / len(self.documents)\n",
    "        self._calculate_idf()\n",
    "        self._index_built = True\n",
    "\n",
    "    def add_document(self, document: Dict[str, Any]):\n",
    "        if not isinstance(document, dict):\n",
    "            raise TypeError(\"Document must be a dictionary.\")\n",
    "        if \"content\" not in document:\n",
    "            raise ValueError(\n",
    "                \"Document dictionary must contain a 'content' key.\"\n",
    "            )\n",
    "\n",
    "        content = document.get(\"content\", \"\")\n",
    "        if not isinstance(content, str):\n",
    "            raise TypeError(\"Document 'content' must be a string.\")\n",
    "\n",
    "        doc_tokens = self._tokenizer(content)\n",
    "\n",
    "        self.documents.append(document)\n",
    "        self._corpus_tokens.append(doc_tokens)\n",
    "        self._update_stats_add(doc_tokens)\n",
    "\n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        if not isinstance(documents, list):\n",
    "            raise TypeError(\"Documents must be a list of dictionaries.\")\n",
    "\n",
    "        if not documents:\n",
    "            return\n",
    "\n",
    "        for i, doc in enumerate(documents):\n",
    "            if not isinstance(doc, dict):\n",
    "                raise TypeError(f\"Document at index {i} must be a dictionary.\")\n",
    "            if \"content\" not in doc:\n",
    "                raise ValueError(\n",
    "                    f\"Document at index {i} must contain a 'content' key.\"\n",
    "                )\n",
    "            if not isinstance(doc[\"content\"], str):\n",
    "                raise TypeError(\n",
    "                    f\"Document 'content' at index {i} must be a string.\"\n",
    "                )\n",
    "\n",
    "            content = doc[\"content\"]\n",
    "            doc_tokens = self._tokenizer(content)\n",
    "\n",
    "            self.documents.append(doc)\n",
    "            self._corpus_tokens.append(doc_tokens)\n",
    "            self._update_stats_add(doc_tokens)\n",
    "\n",
    "        self._index_built = False\n",
    "\n",
    "    def _compute_bm25_score(\n",
    "        self, query_tokens: List[str], doc_index: int\n",
    "    ) -> float:\n",
    "        score = 0.0\n",
    "        doc_term_counts = Counter(self._corpus_tokens[doc_index])\n",
    "        doc_length = self._doc_len[doc_index]\n",
    "\n",
    "        for token in query_tokens:\n",
    "            if token not in self._idf:\n",
    "                continue\n",
    "\n",
    "            idf = self._idf[token]\n",
    "            term_freq = doc_term_counts.get(token, 0)\n",
    "\n",
    "            numerator = idf * term_freq * (self.k1 + 1)\n",
    "            denominator = term_freq + self.k1 * (\n",
    "                1 - self.b + self.b * (doc_length / self._avg_doc_len)\n",
    "            )\n",
    "            score += numerator / (denominator + 1e-9)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: Any,\n",
    "        k: int = 1,\n",
    "        score_normalization_factor: float = 0.1,\n",
    "    ) -> List[Tuple[Dict[str, Any], float]]:\n",
    "        if not self.documents:\n",
    "            return []\n",
    "\n",
    "        if isinstance(query, str):\n",
    "            query_text = query\n",
    "        else:\n",
    "            raise TypeError(\"Query must be a string for BM25Index.\")\n",
    "\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k must be a positive integer.\")\n",
    "\n",
    "        if not self._index_built:\n",
    "            self._build_index()\n",
    "\n",
    "        if self._avg_doc_len == 0:\n",
    "            return []\n",
    "\n",
    "        query_tokens = self._tokenizer(query_text)\n",
    "        if not query_tokens:\n",
    "            return []\n",
    "\n",
    "        raw_scores = []\n",
    "        for i in range(len(self.documents)):\n",
    "            raw_score = self._compute_bm25_score(query_tokens, i)\n",
    "            if raw_score > 1e-9:\n",
    "                raw_scores.append((raw_score, self.documents[i]))\n",
    "\n",
    "        raw_scores.sort(key=lambda item: item[0], reverse=True)\n",
    "\n",
    "        normalized_results = []\n",
    "        for raw_score, doc in raw_scores[:k]:\n",
    "            normalized_score = math.exp(-score_normalization_factor * raw_score)\n",
    "            normalized_results.append((doc, normalized_score))\n",
    "\n",
    "        normalized_results.sort(key=lambda item: item[1])\n",
    "\n",
    "        return normalized_results\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"BM25VectorStore(count={len(self)}, k1={self.k1}, b={self.b}, index_built={self._index_built})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad27c14",
   "metadata": {},
   "source": [
    "# 5. hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6697b0",
   "metadata": {},
   "source": [
    "## Retriever implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c17e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchIndex(Protocol):\n",
    "    def add_document(self, document: Dict[str, Any]) -> None: ...\n",
    "\n",
    "    # Added the 'add_documents' method to avoid rate limiting errors from VoyageAI\n",
    "    def add_documents(self, documents: List[Dict[str, Any]]) -> None: ...\n",
    "\n",
    "    def search(\n",
    "        self, query: Any, k: int = 1\n",
    "    ) -> List[Tuple[Dict[str, Any], float]]: ...\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *indexes: SearchIndex,\n",
    "        reranker_fn: Optional[\n",
    "            Callable[[List[Dict[str, Any]], str, int], List[str]]\n",
    "        ] = None,\n",
    "    ):\n",
    "        if len(indexes) == 0:\n",
    "            raise ValueError(\"At least one index must be provided\")\n",
    "        self._indexes = list(indexes)\n",
    "        self._reranker_fn = reranker_fn\n",
    "\n",
    "    def add_document(self, document: Dict[str, Any]):\n",
    "        if \"id\" not in document:\n",
    "            document[\"id\"] = \"\".join(\n",
    "                random.choices(string.ascii_letters + string.digits, k=4)\n",
    "            )\n",
    "\n",
    "        for index in self._indexes:\n",
    "            index.add_document(document)\n",
    "\n",
    "    # Added the 'add_documents' method to avoid rate limiting errors from VoyageAI\n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        for index in self._indexes:\n",
    "            index.add_documents(documents)\n",
    "\n",
    "    def search(\n",
    "        self, query_text: str, k: int = 1, k_rrf: int = 60\n",
    "    ) -> List[Tuple[Dict[str, Any], float]]:\n",
    "        if not isinstance(query_text, str):\n",
    "            raise TypeError(\"Query text must be a string.\")\n",
    "        if k <= 0:\n",
    "            raise ValueError(\"k must be a positive integer.\")\n",
    "        if k_rrf < 0:\n",
    "            raise ValueError(\"k_rrf must be non-negative.\")\n",
    "\n",
    "        all_results = [\n",
    "            index.search(query_text, k=k * 5) for index in self._indexes\n",
    "        ]\n",
    "\n",
    "        doc_ranks = {}\n",
    "        for idx, results in enumerate(all_results):\n",
    "            for rank, (doc, _) in enumerate(results):\n",
    "                doc_id = id(doc)\n",
    "                if doc_id not in doc_ranks:\n",
    "                    doc_ranks[doc_id] = {\n",
    "                        \"doc_obj\": doc,\n",
    "                        \"ranks\": [float(\"inf\")] * len(self._indexes),\n",
    "                    }\n",
    "                doc_ranks[doc_id][\"ranks\"][idx] = rank + 1\n",
    "\n",
    "        def calc_rrf_score(ranks: List[float]) -> float:\n",
    "            return sum(1.0 / (k_rrf + r) for r in ranks if r != float(\"inf\"))\n",
    "\n",
    "        scored_docs: List[Tuple[Dict[str, Any], float]] = [\n",
    "            (ranks[\"doc_obj\"], calc_rrf_score(ranks[\"ranks\"]))\n",
    "            for ranks in doc_ranks.values()\n",
    "        ]\n",
    "\n",
    "        filtered_docs = [\n",
    "            (doc, score) for doc, score in scored_docs if score > 0\n",
    "        ]\n",
    "        filtered_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        result = filtered_docs[:k]\n",
    "\n",
    "        if self._reranker_fn is not None:\n",
    "            docs_only = [doc for doc, _ in result]\n",
    "\n",
    "            for doc in docs_only:\n",
    "                if \"id\" not in doc:\n",
    "                    doc[\"id\"] = \"\".join(\n",
    "                        random.choices(\n",
    "                            string.ascii_letters + string.digits, k=4\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            doc_lookup = {doc[\"id\"]: doc for doc in docs_only}\n",
    "            reranked_ids = self._reranker_fn(docs_only, query_text, k)\n",
    "\n",
    "            new_result = []\n",
    "            original_scores = {id(doc): score for doc, score in result}\n",
    "\n",
    "            for doc_id in reranked_ids:\n",
    "                if doc_id in doc_lookup:\n",
    "                    doc = doc_lookup[doc_id]\n",
    "                    score = original_scores.get(id(doc), 0.0)\n",
    "                    new_result.append((doc, score))\n",
    "\n",
    "            result = new_result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d5fe7",
   "metadata": {},
   "source": [
    "# 6. re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6bb3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker_fn(docs, query_text, k):\n",
    "    joined_docs = \"\\n\".join(\n",
    "        [\n",
    "            f\"\"\"\n",
    "        <document>\n",
    "        <document_id>{doc[\"id\"]}</document_id>\n",
    "        <document_content>{doc[\"content\"]}</document_content>\n",
    "        </document>\n",
    "        \"\"\"\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = RERANK_PROMPT.format(\n",
    "        k=k,\n",
    "        query_text=query_text,\n",
    "        joined_docs=joined_docs,\n",
    "    )\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "\n",
    "    result = chat(messages, stop_sequences=[\"```\"])\n",
    "\n",
    "    # Note: updated to use 'text_from_message' helper fn\n",
    "    try:\n",
    "        return json.loads(text_from_message(result))[\"document_ids\"]\n",
    "    except:\n",
    "        # Fallback if JSON parsing fails\n",
    "        return [doc[\"id\"] for doc in docs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b5ab8",
   "metadata": {},
   "source": [
    "# 7. contextual answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e965a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add context to a single chunk\n",
    "def add_context(text_chunk, source_text):\n",
    "    prompt = CONTEXT_PROMT.format(\n",
    "        source_text=source_text,\n",
    "        text_chunk=text_chunk,\n",
    "    )\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    add_user_message(messages, prompt)\n",
    "    result = chat(messages)\n",
    "\n",
    "    return text_from_message(result) + \"\\n\" + text_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96413127",
   "metadata": {},
   "source": [
    "# test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb719c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_FILE = \"./assets/report.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30373b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_main():\n",
    "    \"\"\"Test function with a sample query\"\"\"\n",
    "    # Run the setup part of main\n",
    "    with open(DOCUMENT_FILE, \"r\") as f:\n",
    "        source_text = f.read()\n",
    "    \n",
    "    chunks = chunk_by_structure(source_text)\n",
    "    logger.info(f\"Document split into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Add context to chunks\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks[:3]):  # Test with first 3 chunks only\n",
    "        contextualized_chunk = add_context(chunk, source_text)\n",
    "        documents.append({\n",
    "            \"id\": f\"chunk_{i}\",\n",
    "            \"content\": contextualized_chunk\n",
    "        })\n",
    "\n",
    "    # Create embedding function wrapper\n",
    "    def embedding_fn(text_or_texts):\n",
    "        if isinstance(text_or_texts, str):\n",
    "            embeddings = generate_embeddings(text_or_texts)\n",
    "            return embeddings[0].values\n",
    "        else:\n",
    "            all_embeddings = []\n",
    "            for text in text_or_texts:\n",
    "                embeddings = generate_embeddings(text)\n",
    "                all_embeddings.append(embeddings[0].values)\n",
    "            return all_embeddings\n",
    "\n",
    "    # Create retriever\n",
    "    vector_index = VectorIndex(embedding_fn=embedding_fn)\n",
    "    bm25_index = BM25Index()\n",
    "    retriever = Retriever(vector_index, bm25_index, reranker_fn=reranker_fn)\n",
    "    retriever.add_documents(documents)\n",
    "\n",
    "    # Test with a sample query\n",
    "    test_query = \"What are the main findings of the report?\"\n",
    "    print(f\"Testing with query: {test_query}\")\n",
    "    \n",
    "    results = retriever.search(test_query, k=2)\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc, _ in results])\n",
    "    \n",
    "    answer_prompt = ANSWER_PROMT.format(\n",
    "        context=context,\n",
    "        query=test_query\n",
    "    )\n",
    "    \n",
    "    messages = []\n",
    "    add_user_message(messages, answer_prompt)\n",
    "    response = chat(messages)\n",
    "    answer = text_from_message(response)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"TEST RESULT:\")\n",
    "    print(\"=\"*50)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "396a87b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 20:08:06 - __main__ - INFO - Document split into 15 chunks.\n",
      "2025-08-08 20:08:08 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 20:08:09 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 20:08:10 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 20:08:12 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 20:08:13 - __main__ - INFO - Embeddings generated successfully.\n",
      "2025-08-08 20:08:13 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 20:08:13 - __main__ - INFO - Embeddings generated successfully.\n",
      "2025-08-08 20:08:14 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 20:08:14 - __main__ - INFO - Embeddings generated successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with query: What are the main findings of the report?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 20:08:15 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 20:08:15 - __main__ - INFO - Embeddings generated successfully.\n",
      "2025-08-08 20:08:16 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-08 20:08:19 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEST RESULT:\n",
      "==================================================\n",
      "Based on the context provided in the Executive Summary and the Table of Contents, the main findings of the Annual Interdisciplinary Research Review report include:\n",
      "\n",
      "1. Medical Research: Advances in understanding the rare XDR-471 syndrome, leading to new diagnostic insights.\n",
      "\n",
      "2. Software Engineering: Tackled persistent stability issues in Project Phoenix, implementing key fixes identified through error code analysis.\n",
      "\n",
      "3. Financial Analysis: Revealed mixed quarterly performance, prompting strategic reviews, particularly concerning resource allocation impacting R&D pipelines.\n",
      "\n",
      "4. Scientific Experimentation: Characterized novel material properties of Composite XT-5, potentially impacting future product lines.\n",
      "\n",
      "5. Legal Developments: Navigated complex precedents, particularly in intellectual property related to the Synergy Dynamics case, ensuring compliance and mitigating risk.\n",
      "\n",
      "6. Product Engineering: Finalized specifications for the next-generation Model Zircon-5, incorporating feedback from multiple teams.\n",
      "\n",
      "7. Historical Research: Provided unexpected context for current market dynamics through insights into the Galveston Accords (1921).\n",
      "\n",
      "8. Project Management: Successfully navigated critical phases for Project Cerberus despite resource constraints, documented through detailed progress reports.\n",
      "\n",
      "9. Pharmaceutical Development: Advanced Compound CTX-204b into further testing based on promising biomarker results.\n",
      "\n",
      "10. Cybersecurity Analysis: Addressed sophisticated threats, reinforcing defenses based on detailed incident forensics.\n",
      "\n",
      "The report highlights the organization's strength in cross-pollination of ideas and methodologies, driving innovation and addressing complex challenges that transcend traditional disciplinary boundaries.\n"
     ]
    }
   ],
   "source": [
    "test_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0913c",
   "metadata": {},
   "source": [
    "# main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0a24748",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_FILE = \"./assets/report.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca12c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # step 1: text chunking\n",
    "    with open(DOCUMENT_FILE, \"r\") as f:\n",
    "        source_text = f.read()\n",
    "    \n",
    "    chunks = chunk_by_structure(source_text)\n",
    "    logger.info(f\"Document split into {len(chunks)} chunks.\")\n",
    "\n",
    "    # step 2: add context to chunks\n",
    "    logger.info(\"Adding context to chunks...\")\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        contextualized_chunk = add_context(chunk, source_text)\n",
    "        documents.append({\n",
    "            \"id\": f\"chunk_{i}\",\n",
    "            \"content\": contextualized_chunk\n",
    "        })\n",
    "    logger.info(f\"Context added to {len(documents)} chunks.\")\n",
    "\n",
    "    # step 3: create embedding function wrapper\n",
    "    def embedding_fn(text_or_texts):\n",
    "        if isinstance(text_or_texts, str):\n",
    "            embeddings = generate_embeddings(text_or_texts)\n",
    "            return embeddings[0].values\n",
    "        else:\n",
    "            all_embeddings = []\n",
    "            for text in text_or_texts:\n",
    "                embeddings = generate_embeddings(text)\n",
    "                all_embeddings.append(embeddings[0].values)\n",
    "            return all_embeddings\n",
    "        \n",
    "    # step 4: create indexes\n",
    "    logger.info(\"Creating vector and BM25 indexes...\")\n",
    "    vector_index = VectorIndex(embedding_fn=embedding_fn)\n",
    "    bm25_index = BM25Index()\n",
    "\n",
    "    # step 5: create retriever\n",
    "    logger.info(\"Creating retriever...\")\n",
    "    retriever = Retriever(vector_index, bm25_index, reranker_fn=reranker_fn)\n",
    "    \n",
    "    # step 6: add documents to retriever\n",
    "    logger.info(\"Adding documents to retriever...\")\n",
    "    retriever.add_documents(documents)\n",
    "    logger.info(f\"Added {len(documents)} documents to retriever.\")\n",
    "\n",
    "    # step 7: interactive query loop\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RAG SYSTEM READY\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"You can now ask questions about the document.\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Enter your question: \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"USER QUERY:\")\n",
    "        print(\"=\"*50)\n",
    "        print(query)\n",
    "        print()\n",
    "        \n",
    "        # Search for relevant documents\n",
    "        print(\"Searching for relevant information...\")\n",
    "        results = retriever.search(query, k=3)  # Get top 3 most relevant chunks\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No relevant information found.\")\n",
    "            continue\n",
    "            \n",
    "        # Generate answer using retrieved context\n",
    "        context = \"\\n\\n\".join([doc[\"content\"] for doc, _ in results])\n",
    "        \n",
    "        answer_prompt = ANSWER_PROMT.format(\n",
    "            context=context,\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "        messages = []\n",
    "        add_user_message(messages, answer_prompt)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"ASSISTANT RESPONSE:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        response = chat(messages)\n",
    "        answer = text_from_message(response)\n",
    "        print(answer)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604df488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HelloWorld-Claude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
