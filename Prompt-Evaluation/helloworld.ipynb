{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b0d890",
   "metadata": {},
   "source": [
    "# dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02600165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL=\"claude-3-haiku-20240307\"\n",
    "TEMPERATURE=0.7\n",
    "\n",
    "try:\n",
    "    client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Anthropic client: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9ee45",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aca9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    \n",
    "def add_assistant_message(messages, text):\n",
    "    messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "\n",
    "def chat(messages, system_prompt=\"\", stop_sequences=[]):\n",
    "    message = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=1000,\n",
    "        temperature=TEMPERATURE,\n",
    "        system=system_prompt,\n",
    "        messages=messages,\n",
    "        stop_sequences=stop_sequences\n",
    "    )\n",
    "\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2be59f",
   "metadata": {},
   "source": [
    "# generate evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25274fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\",\n",
    "        \"solution_criteria\": \"Key criteria for evaluating the solution\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"Here are some tasks that can be solved with Python, JSON, or Regex for AWS-related tasks: ```json\")\n",
    "    res = chat(messages=messages, stop_sequences=[\"```\"])\n",
    "    \n",
    "    with open(\"aws_tasks.json\", \"w\") as f:\n",
    "        json.dump(json.loads(res), f, indent=2)\n",
    "\n",
    "    logger.info(\"Dataset generated and saved to aws_tasks.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc716e1",
   "metadata": {},
   "source": [
    "# Graders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08dd7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(test_case):\n",
    "    \"\"\"Merges the prompt and test case input, then returns the result\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Please solve the following task:\n",
    "\n",
    "    {test_case[\"task\"]}\n",
    "\n",
    "    * Respond only with Python, JSON, or a plain Regex\n",
    "    * Do not add any comments or commentary or explanation\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    res = chat(messages=messages)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0fa23",
   "metadata": {},
   "source": [
    "## Model Grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef05b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_by_model(test_case, output):\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "    Original Task:\n",
    "    <task>\n",
    "    {test_case[\"task\"]}\n",
    "    </task>\n",
    "\n",
    "    Solution to Evaluate:\n",
    "    <solution>\n",
    "    {output}\n",
    "    </solution>\n",
    "\n",
    "    Criteria you should use to evaluate the solution:\n",
    "    <criteria>\n",
    "    {test_case[\"solution_criteria\"]}\n",
    "    </criteria>\n",
    "\n",
    "    Output Format\n",
    "    Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "    - \"strengths\": An array of 1-3 key strengths\n",
    "    - \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "    - \"reasoning\": A concise explanation of your overall assessment\n",
    "    - \"score\": A number between 1-10\n",
    "\n",
    "    Respond with JSON. Keep your response concise and direct.\n",
    "    Example response shape:\n",
    "    {{\n",
    "        \"strengths\": string[],\n",
    "        \"weaknesses\": string[],\n",
    "        \"reasoning\": string,\n",
    "        \"score\": number\n",
    "    }}\n",
    "    \"\"\"         \n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, evaluation_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages=messages, stop_sequences=[\"```\"])\n",
    "\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0029efd",
   "metadata": {},
   "source": [
    "## Code grader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7cbdf",
   "metadata": {},
   "source": [
    "### Functions to validate the output structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ec1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_by_code(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad80544",
   "metadata": {},
   "source": [
    "# combined evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9673558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    \n",
    "    output = run_prompt(test_case)\n",
    "    \n",
    "    model_grader_res = grade_by_model(test_case=test_case, output=output)\n",
    "    logger.info(\"model grading successful.\")\n",
    "\n",
    "    code_grader_res = grade_by_code(response=output, test_case=test_case)\n",
    "    logger.info(\"code grading successful.\")\n",
    "\n",
    "    score = (model_grader_res[\"score\"] + code_grader_res) / 2\n",
    "    \n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score,\n",
    "        \"strengths\": model_grader_res[\"strengths\"],\n",
    "        \"weaknesses\": model_grader_res[\"weaknesses\"],\n",
    "        \"reasoning\": model_grader_res[\"reasoning\"]\n",
    "    }\n",
    "\n",
    "def run_evaluation(dataset_file=\"aws_tasks.json\"):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "\n",
    "    from statistics import mean\n",
    "    \n",
    "    with open(dataset_file, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    results = []\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    avgerage_score = mean([result[\"score\"] for result in results])\n",
    "    results.append({\n",
    "        \"average_score\": avgerage_score\n",
    "    })\n",
    "\n",
    "    with open(\"evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    logger.info(\"Evaluation completed and results saved to 'evaluation_results.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0e91c",
   "metadata": {},
   "source": [
    "# main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f5e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the evaluation\"\"\"\n",
    "    \n",
    "    dataset_file = \"aws_tasks.json\"\n",
    "    if not os.path.exists(dataset_file):\n",
    "        logger.info(f\"Dataset file '{dataset_file}' not found. Generating dataset...\")\n",
    "        generate_dataset()\n",
    "    \n",
    "    run_evaluation(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050c705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 00:37:26 - __main__ - INFO - Dataset file 'aws_tasks.json' not found. Generating dataset...\n",
      "2025-08-07 00:37:29 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 00:37:29 - __main__ - INFO - Dataset generated and saved to aws_tasks.json\n",
      "2025-08-07 00:37:30 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 00:37:31 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 00:37:31 - __main__ - INFO - model grading successful.\n",
      "2025-08-07 00:37:31 - __main__ - INFO - code grading successful.\n",
      "2025-08-07 00:37:34 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 00:37:35 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 00:37:35 - __main__ - INFO - model grading successful.\n",
      "2025-08-07 00:37:35 - __main__ - INFO - code grading successful.\n",
      "2025-08-07 00:37:37 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 00:37:39 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-08-07 00:37:39 - __main__ - INFO - model grading successful.\n",
      "2025-08-07 00:37:39 - __main__ - INFO - code grading successful.\n",
      "2025-08-07 00:37:39 - __main__ - INFO - Evaluation completed and results saved to 'evaluation_results.json'.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45995e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HelloWorld-Claude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
